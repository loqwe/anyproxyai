# Cursor æµå¼å“åº”è°ƒè¯•æŒ‡å—

## ğŸ› å½“å‰é—®é¢˜

**ç—‡çŠ¶**ï¼š
- Cursor è¯·æ±‚æ˜¾ç¤º `tokens=0`
- åç«¯ï¼ˆAntigravityï¼‰æ˜¾ç¤ºè¯·æ±‚æˆåŠŸå¹¶è¿”å›æ•°æ®
- æµå¼å“åº”å¯èƒ½æ²¡æœ‰æ­£ç¡®ä¼ é€’åˆ° Cursor

## âœ… å·²æ·»åŠ çš„è°ƒè¯•æ—¥å¿—

åœ¨ `internal/service/proxy_service.go` çš„ `streamDirect` å‡½æ•°ä¸­æ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—ï¼š

1. **æµé‡ç»Ÿè®¡**ï¼šè®°å½•ä¼ è¾“çš„æ€»å­—èŠ‚æ•°
2. **å“åº”ç¼“å†²**ï¼šæ˜¾ç¤ºå“åº”ç¼“å†²åŒºçš„å¤§å°
3. **å“åº”é¢„è§ˆ**ï¼šæ˜¾ç¤ºå‰500å­—ç¬¦çš„å“åº”å†…å®¹ï¼ˆdebugçº§åˆ«ï¼‰
4. **Tokenæå–**ï¼šè®°å½•æå–åˆ°çš„ token æ•°é‡

## ğŸ§ª æµ‹è¯•æ­¥éª¤

### 1. å¯ç”¨ Debug æ—¥å¿—çº§åˆ«

åœ¨ `main.go` ä¸­ä¿®æ”¹æ—¥å¿—çº§åˆ«ï¼ˆå¦‚æœéœ€è¦çœ‹åˆ° Debug æ—¥å¿—ï¼‰ï¼š

```go
// ç¬¬94è¡Œï¼Œå°† InfoLevel æ”¹ä¸º DebugLevel
log.SetLevel(log.DebugLevel)  // åŸæ¥æ˜¯ log.InfoLevel
```

### 2. é‡æ–°è¿è¡Œç¨‹åº

```powershell
# é‡æ–°ç¼–è¯‘å¹¶è¿è¡Œ
go run .
```

### 3. å‘é€æµ‹è¯•è¯·æ±‚

ä» Cursor å‘é€ä¸€ä¸ªè¯·æ±‚ï¼Œç„¶åæŸ¥çœ‹æ—¥å¿—è¾“å‡ºã€‚

### 4. æŸ¥çœ‹å…³é”®æ—¥å¿—

åº”è¯¥ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„æ—¥å¿—ï¼š

```
time="..." level=info msg="[Cursor Stream] Received request for model: proxy_auto"
time="..." level=info msg="[Cursor Stream] Routing to: http://127.0.0.1:8545/v1/chat/completions ..."
time="..." level=debug msg="[Stream Direct] Stream completed. Total bytes: 12345"
time="..." level=debug msg="[Stream Direct] Response buffer length: 12345 bytes"
time="..." level=debug msg="[Stream Direct] Response preview: data: {\"id\":\"...\", ..."
time="..." level=info msg="[Stream Direct] Extracted tokens: prompt=100, completion=50, total=150"
time="..." level=info msg="LogRequest: model=claude-opus-4-5-20251101, tokens=150, success=true"
```

## ğŸ” è¯Šæ–­è¦ç‚¹

### å¦‚æœçœ‹åˆ° `tokens=0`

æ£€æŸ¥ä»¥ä¸‹å†…å®¹ï¼š

1. **å“åº”é¢„è§ˆä¸­æ˜¯å¦åŒ…å« `usage` å­—æ®µ**ï¼Ÿ
   - OpenAI æ ¼å¼ï¼š`"usage": {"prompt_tokens": N, "completion_tokens": M}`
   - Claude æ ¼å¼ï¼šåœ¨ `message_start` æˆ– `message_delta` äº‹ä»¶ä¸­

2. **å“åº”æ˜¯å¦å®Œæ•´**ï¼Ÿ
   - æ£€æŸ¥ `Total bytes` å’Œ `Response buffer length` æ˜¯å¦ä¸€è‡´
   - ç¡®è®¤æ²¡æœ‰ "Stream error" æ—¥å¿—

3. **SSE æ ¼å¼æ˜¯å¦æ­£ç¡®**ï¼Ÿ
   - åº”è¯¥æ˜¯ `data: {...}\n\n` æ ¼å¼
   - æ£€æŸ¥æ˜¯å¦æœ‰ `[DONE]` æ ‡è®°

### å¦‚æœå“åº”ä¸­æ²¡æœ‰ usage ä¿¡æ¯

å¯èƒ½çš„åŸå› ï¼š

1. **åç«¯æ²¡æœ‰å‘é€ usage**ï¼š
   - æŸäº› OpenAI å…¼å®¹ API å¯èƒ½ä¸ä¼šåœ¨æµå¼æ¨¡å¼ä¸‹å‘é€ usage
   - éœ€è¦åœ¨è¯·æ±‚ä¸­æ·»åŠ  `stream_options: {include_usage: true}`

2. **æ ¼å¼ä¸åŒ¹é…**ï¼š
   - åç«¯å¯èƒ½ä½¿ç”¨äº†éæ ‡å‡†çš„æ ¼å¼
   - éœ€è¦è°ƒæ•´ `extractTokensFromStreamResponse` å‡½æ•°

## ğŸ› ï¸ å¯èƒ½çš„ä¿®å¤æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: ç¡®ä¿è¯·æ±‚ usage ä¿¡æ¯

åœ¨ Cursor æµå¼è¯·æ±‚ä¸­æ·»åŠ  `stream_options`ï¼ˆç¬¬3912è¡Œé™„è¿‘ï¼‰ï¼š

```go
else {
    reqData["stream"] = true
    reqData["stream_options"] = map[string]interface{}{
        "include_usage": true,
    }
    transformedBody, _ = json.Marshal(reqData)
    targetURL = buildOpenAIChatURL(route.APIUrl)
}
```

### æ–¹æ¡ˆ 2: ä»éæµå¼å­—æ®µæå– tokens

å¦‚æœåç«¯åœ¨æ¯ä¸ª chunk ä¸­éƒ½åŒ…å«ä¸´æ—¶çš„ token è®¡æ•°ï¼Œä¿®æ”¹ `extractTokensFromStreamResponse` æ¥ç´¯ç§¯è¿™äº›å€¼ã€‚

### æ–¹æ¡ˆ 3: Post-hoc Token è®¡æ•°

å¦‚æœå®åœ¨æ— æ³•ä»æµå¼å“åº”ä¸­è·å–ï¼Œå¯ä»¥è€ƒè™‘ï¼š
- ä½¿ç”¨ tiktoken åº“ä¼°ç®— token æ•°ï¼ˆéœ€è¦æ·»åŠ ä¾èµ–ï¼‰
- æˆ–è€…åœ¨éæµå¼æ¨¡å¼ä¸‹è·å–å‡†ç¡®çš„ token æ•°

## ğŸ“Š é¢„æœŸç»“æœ

ä¿®å¤ååº”è¯¥çœ‹åˆ°ï¼š

```
time="..." level=info msg="[Stream Direct] Extracted tokens: prompt=XXX, completion=YYY, total=ZZZ"
time="..." level=info msg="LogRequest: model=..., tokens=ZZZ, success=true"
time="..." level=info msg="POST /api/cursor/v1/chat/completions 200"
```

å…¶ä¸­ XXXã€YYYã€ZZZ éƒ½æ˜¯å¤§äº 0 çš„æ•°å­—ã€‚

## ğŸ“ ä¸‹ä¸€æ­¥

1. è¿è¡Œæµ‹è¯•ï¼Œæ”¶é›†æ—¥å¿—
2. æ ¹æ®æ—¥å¿—è¾“å‡ºç¡®å®šæ ¹æœ¬åŸå› 
3. åº”ç”¨ç›¸åº”çš„ä¿®å¤æ–¹æ¡ˆ
4. éªŒè¯ä¿®å¤æ•ˆæœ

---

**è°ƒè¯•å®Œæˆåè®°å¾—**ï¼šå°†æ—¥å¿—çº§åˆ«æ”¹å› `log.InfoLevel` ä»¥å‡å°‘æ—¥å¿—è¾“å‡ºã€‚
